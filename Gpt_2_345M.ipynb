{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2 345M by Cyberpunk",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quinnpertuit1/gpt-2/blob/master/Gpt_2_345M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA3oRLxMRohz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# medium (345M parameter) versions of GPT-2\n",
        "!cd /content && rm -rf gpt-2 && git clone https://github.com/quinnpertuit1/gpt-2.git\n",
        "%cd /content/gpt-2\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7I5IS2UjAfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 /content/gpt-2/download_model.py 345M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRt4ysIVtdij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 /content/gpt-2/download_model.py 117M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJEhHBWJx0fg",
        "colab_type": "text"
      },
      "source": [
        "## Conditional sample generation\n",
        "​\n",
        "To generate conditional samples from the small model:\n",
        "```\n",
        "!python3 src/interactive_conditional_samples.py\n",
        "```\n",
        "It comes with a few flags available, with a default value: \n",
        "- `seed = None`  || a random value is generated unless specified. give a specific integer value if you want to reproduce same results in the future.\n",
        "- `nsamples = 1`     ||  specify the number of samples you want to print\n",
        "- `length = None  `     ||   number of tokens (words) to print on each sample.\n",
        "- `batch_size = 1`    || how many inputs you want to process simultaneously. doesn't seem to affect the results.\n",
        "- `temperature = 1`   || scales logits before sampling prior to softmax.\n",
        "- `top_k = 0`   ||  truncates the set of logits considered to those with the highest values.\n",
        "​\n",
        "​\n",
        "​\n",
        "The authors tested the model performance on a few different language tasks, including **reading comprehension, text completion, summarization, translation, and question-answering.**\n",
        "​\n",
        "Below are a few examples selected to test the aforementioned behaviors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU49UlE_-S5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --model_name='345M' --nsamples=3 --top_k=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb7sMdJBuzl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --model_name='345M' --nsamples=3 --top_k=20 --temperature=.70"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}